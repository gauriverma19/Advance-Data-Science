{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_emoji.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INXbHUs1vjSF"
      },
      "source": [
        "# **MINI PROJECT 2**\n",
        "\n",
        "Gauri Verma\n",
        "\n",
        "NUID: 001306996\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtoKz_0uvsnv"
      },
      "source": [
        "# ABSTRACT\n",
        "\n",
        "Emojis or avatars are ways to indicate nonverbal cues. These cues have become an essential part of online chatting, product review, brand emotion, and many more. It also lead to increasing data science research dedicated to emoji-driven storytelling.\n",
        "\n",
        "With advancements in computer vision and deep learning, it is now possible to detect human emotions from images. In this deep learning project, we will classify human facial expressions to filter and map corresponding emojis or avatars.\n",
        "\n",
        "ABOUT THE DATASET:\n",
        "\n",
        "The FER2013 dataset ( facial expression recognition) consists of 48*48 pixel grayscale face images. The images are centered and occupy an equal amount of space. This dataset consist of facial emotions of following categories:\n",
        "\n",
        "0:angry\n",
        "\n",
        "1:disgust\n",
        "\n",
        "2:feat\n",
        "\n",
        "3:happy\n",
        "\n",
        "4:sad\n",
        "\n",
        "5:surprise\n",
        "\n",
        "6:natural\n",
        "\n",
        "\n",
        "The dataset can be downloaded from https://www.kaggle.com/msambare/fer2013"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCLiH4ZUwFrG"
      },
      "source": [
        "We will build a deep learning model to classify facial expressions from the images. Then we will map the classified emotion to an emoji or an avatar.\n",
        "\n",
        "Facial Emotion Recognition using CNN:\n",
        "\n",
        "In the below steps will build a convolution neural network architecture and train the model on FER2013 dataset for Emotion recognition from images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnuGJYoEcbQN"
      },
      "source": [
        "#importing necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMHeWHQ5dCjs"
      },
      "source": [
        "#reading train and test file and rescaling the pixels\n",
        "\n",
        "train_dir = 'C://Users//adhar//Downloads//train-20201114T034150Z-001//train'\n",
        "val_dir = 'C://Users//adhar//Downloads//test-20201114T034146Z-001//test'\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJiGes4adFsY",
        "outputId": "9f0f1b2a-d404-40c4-e9c0-512a5d23ab28"
      },
      "source": [
        "#generating directory of training files\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(48,48),\n",
        "        batch_size=64,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 28709 images belonging to 7 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilWYLBGUept-",
        "outputId": "da244913-9002-4bce-c91b-ba54f793c3f1"
      },
      "source": [
        "#generating directory of test files\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=(48,48),\n",
        "        batch_size=64,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 7178 images belonging to 7 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MRIJwiJepqw"
      },
      "source": [
        "#defining the sequential keras model\n",
        "\n",
        "emotion_model = Sequential()\n",
        "\n",
        "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
        "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "emotion_model.add(Dropout(0.25))\n",
        "\n",
        "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "emotion_model.add(Dropout(0.25))\n",
        "\n",
        "emotion_model.add(Flatten())\n",
        "emotion_model.add(Dense(1024, activation='relu'))\n",
        "emotion_model.add(Dropout(0.5))\n",
        "emotion_model.add(Dense(7, activation='softmax'))\n",
        "# emotion_model.load_weights('emotion_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwK2Rz2iwYvo"
      },
      "source": [
        "# Conv2d\n",
        "\n",
        "Keras Conv2D is a 2D Convolution Layer, this layer creates a convolution kernel that is wind with layers input which helps produce a tensor of outputs.\n",
        "\n",
        "Kernel: In image processing kernel is a convolution matrix or masks which can be used for blurring, sharpening, embossing, edge detection, and more by doing a convolution between a kernel and an image.\n",
        "\n",
        "* Mandatory Conv2D parameter is the numbers of filters that convolutional layers will learn from.\n",
        "It is an integer value and also determines the number of output filters in the convolution.\n",
        "\n",
        "* Here we are learning a total of 32 filters and then we use Max Pooling to reduce the spatial dimensions of the output volume.As far as choosing the appropriate value for no. of filters, it is always recommended to use powers of 2 as the values\n",
        "\n",
        "* Kernel Size:  This parameter determines the dimensions of the kernel. Common dimensions include 1×1, 3×3, 5×5, and 7×7 which can be passed as (1, 1), (3, 3), (5, 5), or (7, 7) tuples.\n",
        "It is an integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window.\n",
        "This parameter must be an odd integer.\n",
        "\n",
        "* Strides: This parameter is an integer or tuple/list of 2 integers, specifying the “step” of the convolution along with the height and width of the input volume.\n",
        "Its default value is always set to (1, 1) which means that the given Conv2D filter is applied to the current location of the input volume and the given filter takes a 1-pixel step to the right and again the filter is applied to the input volume and it is performed until we reach the far right border of the volume in which we are moving our filter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGvDp_jwwaih"
      },
      "source": [
        "# RELU Activation\n",
        "\n",
        "* In a neural network, the activation function is responsible for transforming the summed weighted input from the node into the activation of the node or output for that input.\n",
        "\n",
        "* The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHYB5WRiwfjS"
      },
      "source": [
        "# MaxPool2D\n",
        "\n",
        "* In essence it scales down the matrix. You will still have the same data, but scaled down version. How much it scales is determined by stride parameter.\n",
        "\n",
        "* Kernel size (a small window to look) determines the area to “pool” over and stride determines the step. Imagine, it starts applying kernel from upper left corner and moves the kernel by stride. So kernel 1x1 and stride 1 does nothing, keeps the input. Kernel 2x2, stride 2 will shrink the data by 2. Shrinking effect comes from the stride parameter (a step to take). Kernel 1x1, stride 2 will also shrink the data by 2, but will just keep every second pixel while 2x2 kernel will keep the max pixel from the 2x2 area"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C1cSiNIwh9B"
      },
      "source": [
        "# Dense\n",
        "\n",
        "* Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNZQjz_xfDTX",
        "outputId": "6bde4d50-077b-4578-8058-bdf8ac695e7f"
      },
      "source": [
        "#using cv2 and creating dictionary of 7 human emotions\n",
        "\n",
        "cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
        "\n",
        "#compiling the model using categorical crossentropy and adam optimizer with a learning rate of 0.0001\n",
        "\n",
        "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
        "emotion_model_info = emotion_model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=28709 // 64,\n",
        "        epochs=100,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=7178 // 64)\n",
        "\n",
        "#saving model weights to be used in the gui file next\n",
        "\n",
        "emotion_model.save_weights('emotion_model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-507a0032e3c9>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/100\n",
            "448/448 [==============================] - 154s 343ms/step - loss: 1.8076 - accuracy: 0.2546 - val_loss: 1.7429 - val_accuracy: 0.3061\n",
            "Epoch 2/100\n",
            "448/448 [==============================] - 78s 174ms/step - loss: 1.6515 - accuracy: 0.3547 - val_loss: 1.5656 - val_accuracy: 0.4124\n",
            "Epoch 3/100\n",
            "448/448 [==============================] - 78s 175ms/step - loss: 1.5524 - accuracy: 0.4008 - val_loss: 1.4870 - val_accuracy: 0.4376\n",
            "Epoch 4/100\n",
            "448/448 [==============================] - 77s 171ms/step - loss: 1.4824 - accuracy: 0.4315 - val_loss: 1.4241 - val_accuracy: 0.4605\n",
            "Epoch 5/100\n",
            "448/448 [==============================] - 76s 171ms/step - loss: 1.4238 - accuracy: 0.4532 - val_loss: 1.3701 - val_accuracy: 0.4791\n",
            "Epoch 6/100\n",
            "448/448 [==============================] - 77s 171ms/step - loss: 1.3681 - accuracy: 0.4820 - val_loss: 1.3302 - val_accuracy: 0.4929\n",
            "Epoch 7/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 1.3246 - accuracy: 0.4992 - val_loss: 1.3035 - val_accuracy: 0.5033\n",
            "Epoch 8/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 1.2801 - accuracy: 0.5140 - val_loss: 1.2534 - val_accuracy: 0.5216\n",
            "Epoch 9/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 1.2439 - accuracy: 0.5296 - val_loss: 1.2295 - val_accuracy: 0.5317\n",
            "Epoch 10/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 1.2108 - accuracy: 0.5430 - val_loss: 1.2060 - val_accuracy: 0.5416\n",
            "Epoch 11/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 1.1823 - accuracy: 0.5567 - val_loss: 1.1902 - val_accuracy: 0.5453\n",
            "Epoch 12/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 1.1526 - accuracy: 0.5673 - val_loss: 1.1681 - val_accuracy: 0.5645\n",
            "Epoch 13/100\n",
            "448/448 [==============================] - 77s 172ms/step - loss: 1.1266 - accuracy: 0.5753 - val_loss: 1.1540 - val_accuracy: 0.5638\n",
            "Epoch 14/100\n",
            "448/448 [==============================] - 79s 175ms/step - loss: 1.1003 - accuracy: 0.5877 - val_loss: 1.1540 - val_accuracy: 0.5649\n",
            "Epoch 15/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 1.0803 - accuracy: 0.5945 - val_loss: 1.1280 - val_accuracy: 0.5727\n",
            "Epoch 16/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 1.0582 - accuracy: 0.6057 - val_loss: 1.1169 - val_accuracy: 0.5795\n",
            "Epoch 17/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 1.0322 - accuracy: 0.6165 - val_loss: 1.1262 - val_accuracy: 0.5738\n",
            "Epoch 18/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 1.0100 - accuracy: 0.6258 - val_loss: 1.0950 - val_accuracy: 0.5884\n",
            "Epoch 19/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.9867 - accuracy: 0.6325 - val_loss: 1.0892 - val_accuracy: 0.5911\n",
            "Epoch 20/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.9698 - accuracy: 0.6406 - val_loss: 1.0915 - val_accuracy: 0.5905\n",
            "Epoch 21/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.9451 - accuracy: 0.6511 - val_loss: 1.0868 - val_accuracy: 0.5957\n",
            "Epoch 22/100\n",
            "448/448 [==============================] - 77s 172ms/step - loss: 0.9217 - accuracy: 0.6575 - val_loss: 1.0718 - val_accuracy: 0.5979\n",
            "Epoch 23/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.9000 - accuracy: 0.6673 - val_loss: 1.0724 - val_accuracy: 0.6048\n",
            "Epoch 24/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.8860 - accuracy: 0.6721 - val_loss: 1.0810 - val_accuracy: 0.5971\n",
            "Epoch 25/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.8559 - accuracy: 0.6853 - val_loss: 1.0698 - val_accuracy: 0.6060\n",
            "Epoch 26/100\n",
            "448/448 [==============================] - 77s 171ms/step - loss: 0.8392 - accuracy: 0.6916 - val_loss: 1.0809 - val_accuracy: 0.6051\n",
            "Epoch 27/100\n",
            "448/448 [==============================] - 77s 171ms/step - loss: 0.8194 - accuracy: 0.7000 - val_loss: 1.0619 - val_accuracy: 0.6060\n",
            "Epoch 28/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.7903 - accuracy: 0.7115 - val_loss: 1.0665 - val_accuracy: 0.6063\n",
            "Epoch 29/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.7676 - accuracy: 0.7198 - val_loss: 1.0595 - val_accuracy: 0.6141\n",
            "Epoch 30/100\n",
            "448/448 [==============================] - 77s 172ms/step - loss: 0.7502 - accuracy: 0.7245 - val_loss: 1.0612 - val_accuracy: 0.6115\n",
            "Epoch 31/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.7311 - accuracy: 0.7381 - val_loss: 1.0697 - val_accuracy: 0.6095\n",
            "Epoch 32/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.7075 - accuracy: 0.7410 - val_loss: 1.0603 - val_accuracy: 0.6138\n",
            "Epoch 33/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.6869 - accuracy: 0.7513 - val_loss: 1.0672 - val_accuracy: 0.6158\n",
            "Epoch 34/100\n",
            "448/448 [==============================] - 76s 171ms/step - loss: 0.6645 - accuracy: 0.7584 - val_loss: 1.0706 - val_accuracy: 0.6184\n",
            "Epoch 35/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.6422 - accuracy: 0.7678 - val_loss: 1.0754 - val_accuracy: 0.6169\n",
            "Epoch 36/100\n",
            "448/448 [==============================] - 76s 169ms/step - loss: 0.6245 - accuracy: 0.7719 - val_loss: 1.0988 - val_accuracy: 0.6164\n",
            "Epoch 37/100\n",
            "448/448 [==============================] - 76s 169ms/step - loss: 0.6088 - accuracy: 0.7792 - val_loss: 1.0855 - val_accuracy: 0.6136\n",
            "Epoch 38/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.5869 - accuracy: 0.7893 - val_loss: 1.1056 - val_accuracy: 0.6184\n",
            "Epoch 39/100\n",
            "448/448 [==============================] - 76s 169ms/step - loss: 0.5639 - accuracy: 0.7973 - val_loss: 1.1016 - val_accuracy: 0.6233\n",
            "Epoch 40/100\n",
            "448/448 [==============================] - 76s 169ms/step - loss: 0.5500 - accuracy: 0.8007 - val_loss: 1.1001 - val_accuracy: 0.6267\n",
            "Epoch 41/100\n",
            "448/448 [==============================] - 77s 173ms/step - loss: 0.5281 - accuracy: 0.8097 - val_loss: 1.1023 - val_accuracy: 0.6223\n",
            "Epoch 42/100\n",
            "448/448 [==============================] - 76s 169ms/step - loss: 0.5128 - accuracy: 0.8149 - val_loss: 1.1143 - val_accuracy: 0.6230\n",
            "Epoch 43/100\n",
            "448/448 [==============================] - 76s 169ms/step - loss: 0.4940 - accuracy: 0.8215 - val_loss: 1.1280 - val_accuracy: 0.6233\n",
            "Epoch 44/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.4823 - accuracy: 0.8253 - val_loss: 1.1221 - val_accuracy: 0.6242\n",
            "Epoch 45/100\n",
            "448/448 [==============================] - 77s 173ms/step - loss: 0.4676 - accuracy: 0.8308 - val_loss: 1.1321 - val_accuracy: 0.6277\n",
            "Epoch 46/100\n",
            "448/448 [==============================] - 77s 171ms/step - loss: 0.4537 - accuracy: 0.8344 - val_loss: 1.1210 - val_accuracy: 0.6302\n",
            "Epoch 47/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.4347 - accuracy: 0.8456 - val_loss: 1.1557 - val_accuracy: 0.6258\n",
            "Epoch 48/100\n",
            "448/448 [==============================] - 76s 171ms/step - loss: 0.4210 - accuracy: 0.8472 - val_loss: 1.1741 - val_accuracy: 0.6260\n",
            "Epoch 49/100\n",
            "448/448 [==============================] - 77s 172ms/step - loss: 0.4085 - accuracy: 0.8520 - val_loss: 1.1751 - val_accuracy: 0.6346\n",
            "Epoch 50/100\n",
            "448/448 [==============================] - 77s 173ms/step - loss: 0.4004 - accuracy: 0.8560 - val_loss: 1.1654 - val_accuracy: 0.6272\n",
            "Epoch 51/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.3886 - accuracy: 0.8598 - val_loss: 1.1879 - val_accuracy: 0.6295\n",
            "Epoch 52/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.3723 - accuracy: 0.8651 - val_loss: 1.1823 - val_accuracy: 0.6270\n",
            "Epoch 53/100\n",
            "448/448 [==============================] - 82s 184ms/step - loss: 0.3661 - accuracy: 0.8685 - val_loss: 1.2268 - val_accuracy: 0.6254\n",
            "Epoch 54/100\n",
            "448/448 [==============================] - 83s 185ms/step - loss: 0.3575 - accuracy: 0.8710 - val_loss: 1.2059 - val_accuracy: 0.6271\n",
            "Epoch 55/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "448/448 [==============================] - 80s 179ms/step - loss: 0.3416 - accuracy: 0.8776 - val_loss: 1.2234 - val_accuracy: 0.6303\n",
            "Epoch 56/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.3296 - accuracy: 0.8821 - val_loss: 1.2565 - val_accuracy: 0.6242\n",
            "Epoch 57/100\n",
            "448/448 [==============================] - 76s 171ms/step - loss: 0.3232 - accuracy: 0.8842 - val_loss: 1.2503 - val_accuracy: 0.6278\n",
            "Epoch 58/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.3196 - accuracy: 0.8866 - val_loss: 1.2479 - val_accuracy: 0.6278\n",
            "Epoch 59/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.2992 - accuracy: 0.8930 - val_loss: 1.2603 - val_accuracy: 0.6318\n",
            "Epoch 60/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.3058 - accuracy: 0.8906 - val_loss: 1.2567 - val_accuracy: 0.6297\n",
            "Epoch 61/100\n",
            "448/448 [==============================] - 77s 172ms/step - loss: 0.2918 - accuracy: 0.8954 - val_loss: 1.2617 - val_accuracy: 0.6297\n",
            "Epoch 62/100\n",
            "448/448 [==============================] - 78s 175ms/step - loss: 0.2817 - accuracy: 0.8997 - val_loss: 1.2975 - val_accuracy: 0.6265\n",
            "Epoch 63/100\n",
            "448/448 [==============================] - 82s 182ms/step - loss: 0.2704 - accuracy: 0.9041 - val_loss: 1.3012 - val_accuracy: 0.6257\n",
            "Epoch 64/100\n",
            "448/448 [==============================] - 78s 175ms/step - loss: 0.2706 - accuracy: 0.9039 - val_loss: 1.2902 - val_accuracy: 0.6254\n",
            "Epoch 65/100\n",
            "448/448 [==============================] - 78s 174ms/step - loss: 0.2667 - accuracy: 0.9058 - val_loss: 1.2936 - val_accuracy: 0.6275\n",
            "Epoch 66/100\n",
            "448/448 [==============================] - 78s 174ms/step - loss: 0.2518 - accuracy: 0.9107 - val_loss: 1.3136 - val_accuracy: 0.6253\n",
            "Epoch 67/100\n",
            "448/448 [==============================] - 77s 171ms/step - loss: 0.2513 - accuracy: 0.9097 - val_loss: 1.3538 - val_accuracy: 0.6299\n",
            "Epoch 68/100\n",
            "448/448 [==============================] - 78s 175ms/step - loss: 0.2425 - accuracy: 0.9144 - val_loss: 1.3306 - val_accuracy: 0.6244\n",
            "Epoch 69/100\n",
            "448/448 [==============================] - 78s 175ms/step - loss: 0.2464 - accuracy: 0.9117 - val_loss: 1.3339 - val_accuracy: 0.6310\n",
            "Epoch 70/100\n",
            "448/448 [==============================] - 79s 175ms/step - loss: 0.2296 - accuracy: 0.9185 - val_loss: 1.3447 - val_accuracy: 0.6239\n",
            "Epoch 71/100\n",
            "448/448 [==============================] - 78s 174ms/step - loss: 0.2323 - accuracy: 0.9163 - val_loss: 1.3433 - val_accuracy: 0.6265\n",
            "Epoch 72/100\n",
            "448/448 [==============================] - 78s 174ms/step - loss: 0.2210 - accuracy: 0.9216 - val_loss: 1.3581 - val_accuracy: 0.6256\n",
            "Epoch 73/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.2209 - accuracy: 0.9216 - val_loss: 1.3607 - val_accuracy: 0.6292\n",
            "Epoch 74/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.2137 - accuracy: 0.9246 - val_loss: 1.3919 - val_accuracy: 0.6331\n",
            "Epoch 75/100\n",
            "448/448 [==============================] - 76s 171ms/step - loss: 0.2130 - accuracy: 0.9241 - val_loss: 1.4013 - val_accuracy: 0.6282\n",
            "Epoch 76/100\n",
            "448/448 [==============================] - 77s 172ms/step - loss: 0.2089 - accuracy: 0.9280 - val_loss: 1.4285 - val_accuracy: 0.6296\n",
            "Epoch 77/100\n",
            "448/448 [==============================] - 76s 171ms/step - loss: 0.2019 - accuracy: 0.9297 - val_loss: 1.3937 - val_accuracy: 0.6317\n",
            "Epoch 78/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.1978 - accuracy: 0.9297 - val_loss: 1.3809 - val_accuracy: 0.6278\n",
            "Epoch 79/100\n",
            "448/448 [==============================] - 76s 171ms/step - loss: 0.1950 - accuracy: 0.9302 - val_loss: 1.4258 - val_accuracy: 0.6277\n",
            "Epoch 80/100\n",
            "448/448 [==============================] - 77s 171ms/step - loss: 0.1919 - accuracy: 0.9322 - val_loss: 1.4253 - val_accuracy: 0.6317\n",
            "Epoch 81/100\n",
            "448/448 [==============================] - 76s 171ms/step - loss: 0.1861 - accuracy: 0.9350 - val_loss: 1.4178 - val_accuracy: 0.6304\n",
            "Epoch 82/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.1866 - accuracy: 0.9326 - val_loss: 1.4239 - val_accuracy: 0.6297\n",
            "Epoch 83/100\n",
            "448/448 [==============================] - 77s 171ms/step - loss: 0.1870 - accuracy: 0.9339 - val_loss: 1.4244 - val_accuracy: 0.6324\n",
            "Epoch 84/100\n",
            "448/448 [==============================] - 77s 171ms/step - loss: 0.1810 - accuracy: 0.9368 - val_loss: 1.4448 - val_accuracy: 0.6311\n",
            "Epoch 85/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.1750 - accuracy: 0.9379 - val_loss: 1.4523 - val_accuracy: 0.6320\n",
            "Epoch 86/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.1790 - accuracy: 0.9379 - val_loss: 1.4286 - val_accuracy: 0.6288\n",
            "Epoch 87/100\n",
            "448/448 [==============================] - 76s 171ms/step - loss: 0.1723 - accuracy: 0.9395 - val_loss: 1.4535 - val_accuracy: 0.6261\n",
            "Epoch 88/100\n",
            "448/448 [==============================] - 77s 172ms/step - loss: 0.1633 - accuracy: 0.9418 - val_loss: 1.4724 - val_accuracy: 0.6243\n",
            "Epoch 89/100\n",
            "448/448 [==============================] - 76s 170ms/step - loss: 0.1661 - accuracy: 0.9406 - val_loss: 1.4644 - val_accuracy: 0.6289\n",
            "Epoch 90/100\n",
            "448/448 [==============================] - 77s 171ms/step - loss: 0.1604 - accuracy: 0.9443 - val_loss: 1.5395 - val_accuracy: 0.6293\n",
            "Epoch 91/100\n",
            "448/448 [==============================] - 77s 171ms/step - loss: 0.1590 - accuracy: 0.9439 - val_loss: 1.5167 - val_accuracy: 0.6267\n",
            "Epoch 92/100\n",
            "448/448 [==============================] - 77s 172ms/step - loss: 0.1563 - accuracy: 0.9456 - val_loss: 1.4620 - val_accuracy: 0.6261\n",
            "Epoch 93/100\n",
            "448/448 [==============================] - 78s 174ms/step - loss: 0.1540 - accuracy: 0.9462 - val_loss: 1.4694 - val_accuracy: 0.6306\n",
            "Epoch 94/100\n",
            "448/448 [==============================] - 78s 174ms/step - loss: 0.1500 - accuracy: 0.9482 - val_loss: 1.4638 - val_accuracy: 0.6316\n",
            "Epoch 95/100\n",
            "448/448 [==============================] - 77s 171ms/step - loss: 0.1530 - accuracy: 0.9457 - val_loss: 1.5573 - val_accuracy: 0.6281\n",
            "Epoch 96/100\n",
            "448/448 [==============================] - 78s 173ms/step - loss: 0.1446 - accuracy: 0.9500 - val_loss: 1.5136 - val_accuracy: 0.6306\n",
            "Epoch 97/100\n",
            "448/448 [==============================] - 79s 176ms/step - loss: 0.1532 - accuracy: 0.9474 - val_loss: 1.5086 - val_accuracy: 0.6317\n",
            "Epoch 98/100\n",
            "448/448 [==============================] - 80s 178ms/step - loss: 0.1452 - accuracy: 0.9508 - val_loss: 1.5359 - val_accuracy: 0.6310\n",
            "Epoch 99/100\n",
            "448/448 [==============================] - 82s 183ms/step - loss: 0.1391 - accuracy: 0.9522 - val_loss: 1.5226 - val_accuracy: 0.6285\n",
            "Epoch 100/100\n",
            "448/448 [==============================] - 91s 203ms/step - loss: 0.1407 - accuracy: 0.9500 - val_loss: 1.5475 - val_accuracy: 0.6295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuNwL2hpKPtM"
      },
      "source": [
        "I have trained the model with 100 epochs on GPU: NVIDIA 2070 SUPER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT1PZZsFwqTX"
      },
      "source": [
        "# Adam Optimizer \n",
        "\n",
        "* An optimizer is one of the two arguments required for compiling a Keras model:\n",
        "* Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
        "* The method is \"computationally efficient, has little memory requirement, invariant to diagonal rescaling of gradients, and is well suited for problems that are large in terms of data/parameters\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQjaXk3nS6Jh"
      },
      "source": [
        "# start the webcam feed\n",
        "cap = cv2.VideoCapture(0)\n",
        "while True:\n",
        "    # Find haar cascade to draw bounding box around face\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    bounding_box = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2gray_frame)\n",
        "    num_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "    for (x, y, w, h) in num_faces:\n",
        "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
        "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
        "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
        "        emotion_prediction = emotion_model.predict(cropped_img)\n",
        "        maxindex = int(np.argmax(emotion_prediction))\n",
        "        cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "    cv2.imshow('Video', cv2.resize(frame,(1200,860),interpolation = cv2.INTER_CUBIC))\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW-fxGnTwOGV"
      },
      "source": [
        "I have built a convolution neural network to recognize facial emotions. We have trained our model on the FER2013 dataset. Then I have mapped those emotions with the corresponding emojis or avatars.\n",
        "\n",
        "Using OpenCV’s haar cascade xml we are getting the bounding box of the faces in the webcam. Then we feed these boxes to the trained model for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF7u8qrkKbNw"
      },
      "source": [
        "The code for the GUI is included in gui_emoji.ipynb file as it needs to run seperately."
      ]
    }
  ]
}